{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "#import cudf\n",
    "#import cupy\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cat\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#cudf.__version__"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-18T08:08:00.236552Z",
     "iopub.execute_input": "2022-08-18T08:08:00.237014Z",
     "iopub.status.idle": "2022-08-18T08:08:00.244340Z",
     "shell.execute_reply.started": "2022-08-18T08:08:00.236951Z",
     "shell.execute_reply": "2022-08-18T08:08:00.243179Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGB+LGBM+CatBoost+CNN\n",
    "\n",
    "Compared with the other two models, the accuracy of catboost is slightly lower, about 0.790. If you are interested, you can try to improve the accuracy of catboost. I believe it will be helpful to the final score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def get_not_used():\n",
    "    # cid is the label encode of customer_ID\n",
    "    # row_id indicates the order of rows\n",
    "    misscols= ['D_88','D_110','B_39','D_73','B_42','D_134','B_29','D_76','D_132','D_42','D_142','D_53']\n",
    "    skew=['B_31', 'D_87']\n",
    "    return ['row_id', 'customer_ID', 'target', 'cid', 'S_2','month']+skew+misscols[:-5]\n",
    "    \n",
    "def preprocess(df):\n",
    "    df['row_id'] = cupy.arange(df.shape[0])\n",
    "    not_used = get_not_used()\n",
    "    cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120',\n",
    "                'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in not_used+cat_cols:\n",
    "            df[col] = df[col].round(2)\n",
    "\n",
    "    df['S_2'] = cudf.to_datetime(df['S_2'])\n",
    "    df['cid'], _ = df.customer_ID.factorize()\n",
    "    df= df.sort_values(['customer_ID','S_2'])\n",
    "    df['month']= df['S_2'].dt.day\n",
    "    df['month'] =df.to_pandas().groupby('customer_ID')['month'].diff()\n",
    "    \n",
    "    important = ['P_2','B_1','B_4','D_39']\n",
    "    num_cols = [col for col in df.columns if col not in cat_cols+not_used]+['month']\n",
    "    nth_cols = [col for col in df.columns if col not in cat_cols+not_used][:30]+['customer_ID']\n",
    "    \n",
    "    dgs = add_stats_step(df, num_cols, cat_cols)\n",
    "        \n",
    "    # cudf merge changes row orders\n",
    "    # restore the original row order by sorting row_id\n",
    "    df= df.merge(df[nth_cols].groupby('customer_ID').nth(-2),on='customer_ID',how='left',suffixes=[\"_last_1\",\"_last_2\"])\n",
    "    df = df.sort_values('row_id')\n",
    "    df = df.drop(['row_id'],axis=1)\n",
    "    return df, dgs\n",
    "\n",
    "def add_stats_step(df, numcols, catcols):\n",
    "    n = 50\n",
    "    dgs = []\n",
    "    for i in range(0,len(numcols),n):\n",
    "        s = i\n",
    "        e = min(s+n, len(numcols))\n",
    "        dg = add_stats_one_shot_num(df, numcols[s:e])\n",
    "        dgs.append(dg)\n",
    "    for i in range(0,len(catcols),n):\n",
    "        s = i\n",
    "        e = min(s+n, len(catcols))\n",
    "        dg = add_stats_one_shot_cat(df, catcols[s:e])\n",
    "        dgs.append(dg)\n",
    "    return dgs\n",
    "\n",
    "def add_stats_one_shot_num(df, cols):\n",
    "    stats = ['mean','max','min']\n",
    "    dg = df.groupby('customer_ID').agg({col:stats for col in cols})\n",
    "    out_cols = []\n",
    "    for col in cols:\n",
    "        out_cols.extend([f'{col}_{s}' for s in stats])\n",
    "    dg.columns = out_cols\n",
    "    dg = dg.reset_index()\n",
    "    return dg\n",
    "\n",
    "def add_stats_one_shot_cat(df, cols):\n",
    "    stats = ['count', 'nunique','mean','last','max','min']\n",
    "    dg = df.groupby('customer_ID').agg({col:stats for col in cols})\n",
    "    out_cols = []\n",
    "    for col in cols:\n",
    "        out_cols.extend([f'{col}_{s}' for s in stats])\n",
    "    dg.columns = out_cols\n",
    "    for col in cols:\n",
    "        df[f'{col}_cat_diff'] =df.to_pandas().groupby('customer_ID')[col].diff().iloc[[-1]]\n",
    "    for col in cols:\n",
    "        df[f'{col}_cat_pct_change'] =df.to_pandas().groupby('customer_ID')[col].pct_change().iloc[[-1]]\n",
    "    for col in cols:\n",
    "        dg[f'{col}_last_mean'] = dg[f'{col}_last'] - dg[f'{col}_mean']\n",
    "    for col in cols:\n",
    "        dg[f'{col}_max_min'] = dg[f'{col}_max'] - dg[f'{col}_min']\n",
    "    dg = dg.reset_index()\n",
    "    return dg\n",
    "\n",
    "def load_test_iter(path, chunks=15):\n",
    "    \n",
    "    test_rows = 11363762\n",
    "    chunk_rows = test_rows // chunks\n",
    "    \n",
    "    test = cudf.read_parquet(f'{path}/test.parquet',\n",
    "                             columns=['customer_ID','S_2'],\n",
    "                             num_rows=test_rows)\n",
    "    test = get_segment(test)\n",
    "    start = 0\n",
    "    while start < test.shape[0]:\n",
    "        if start+chunk_rows < test.shape[0]:\n",
    "            end = test['cus_count'].values[start+chunk_rows]\n",
    "        else:\n",
    "            end = test['cus_count'].values[-1]\n",
    "        end = int(end)\n",
    "        df = cudf.read_parquet(f'{path}/test.parquet',\n",
    "                               num_rows = end-start, skiprows=start)\n",
    "        start = end\n",
    "        yield process_data(df)\n",
    "\n",
    "def load_train(path):\n",
    "    train = cudf.read_parquet(f'{path}/train.parquet')\n",
    "    train = process_data(train)\n",
    "    trainl = cudf.read_csv(f'../input/amex-default-prediction/train_labels.csv')\n",
    "    train = train.merge(trainl, on='customer_ID', how='left')\n",
    "    return train\n",
    "\n",
    "def process_data(df):\n",
    "    df,dgs = preprocess(df)\n",
    "    df = df.drop_duplicates('customer_ID',keep='last')\n",
    "    for dg in dgs:\n",
    "        df = df.merge(dg, on='customer_ID', how='left')\n",
    "#     diff_cols = [col for col in df.columns if col.endswith('_diff')]\n",
    "#     df = df.drop(diff_cols,axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_segment(test):\n",
    "    dg = test.groupby('customer_ID').agg({'S_2':'count'})\n",
    "    dg.columns = ['cus_count']\n",
    "    dg = dg.reset_index()\n",
    "    dg['cid'],_ = dg['customer_ID'].factorize()\n",
    "    dg = dg.sort_values('cid')\n",
    "    dg['cus_count'] = dg['cus_count'].cumsum()\n",
    "    \n",
    "    test = test.merge(dg, on='customer_ID', how='left')\n",
    "    test = test.sort_values(['cid','S_2'])\n",
    "    assert test['cus_count'].values[-1] == test.shape[0]\n",
    "    return test"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-18T08:07:02.987750Z",
     "iopub.status.idle": "2022-08-18T08:07:02.988209Z",
     "shell.execute_reply.started": "2022-08-18T08:07:02.987971Z",
     "shell.execute_reply": "2022-08-18T08:07:02.987988Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def xgb_train(x, y, xt, yt):\n",
    "    print(\"-----------xgb starts training-----------\")\n",
    "    print(\"# of features:\", x.shape[1])\n",
    "    assert x.shape[1] == xt.shape[1]\n",
    "    dtrain = xgb.DMatrix(data=x, label=y)\n",
    "    dvalid = xgb.DMatrix(data=xt, label=yt)\n",
    "    params = {\n",
    "            'objective': 'binary:logistic', \n",
    "            'tree_method': 'gpu_hist', \n",
    "            'max_depth': 7,\n",
    "            'subsample':0.88,\n",
    "            'colsample_bytree': 0.5,\n",
    "            'gamma':1.5,\n",
    "            'min_child_weight':8,\n",
    "            'lambda':70,\n",
    "            'eta':0.03,\n",
    "#             'scale_pos_weight': scale_pos_weight,\n",
    "    }\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    bst = xgb.train(params, dtrain=dtrain,\n",
    "                num_boost_round=20500,evals=watchlist,\n",
    "                early_stopping_rounds=500, feval=xgb_amex, maximize=True,\n",
    "                verbose_eval=100)\n",
    "    print('best ntree_limit:', bst.best_ntree_limit)\n",
    "    print('best score:', bst.best_score)\n",
    "    return bst.predict(dtrain, iteration_range=(0,bst.best_ntree_limit)), bst.predict(dvalid, iteration_range=(0,bst.best_ntree_limit)), bst"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-18T08:07:02.989390Z",
     "iopub.status.idle": "2022-08-18T08:07:02.989794Z",
     "shell.execute_reply.started": "2022-08-18T08:07:02.989597Z",
     "shell.execute_reply": "2022-08-18T08:07:02.989615Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def lgb_train(x, y, xt, yt):\n",
    "    print(\"----------lgb starts training----------\")\n",
    "    print(\"# of features:\", x.shape[1])\n",
    "    assert x.shape[1] == xt.shape[1]\n",
    "    lgb_train = lgb.Dataset(x.to_pandas(), y.to_pandas())\n",
    "    lgb_eval = lgb.Dataset(xt.to_pandas(), yt.to_pandas(), reference=lgb_train)\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting':'gbdt',\n",
    "        'seed': 42,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40\n",
    "       \n",
    "    }\n",
    "    gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=20500,\n",
    "                valid_sets=[lgb_train, lgb_eval],\n",
    "                early_stopping_rounds=500,feval=amex_metric_mod_lgbm, \n",
    "                verbose_eval=100,)\n",
    "\n",
    "\n",
    "    print('best iterations:', gbm.best_iteration)\n",
    "    print('best score:', gbm.best_score)\n",
    "    return gbm.predict(x.to_pandas(), num_iteration =gbm.best_iteration),gbm.predict(xt.to_pandas(), num_iteration =gbm.best_iteration), gbm"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-18T08:07:02.991141Z",
     "iopub.status.idle": "2022-08-18T08:07:02.991509Z",
     "shell.execute_reply.started": "2022-08-18T08:07:02.991330Z",
     "shell.execute_reply": "2022-08-18T08:07:02.991347Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def cat_train(x, y, xt, yt):\n",
    "    print(\"-----------catboost starts training-----------\")\n",
    "    print(\"# of features:\", x.shape[1])\n",
    "    assert x.shape[1] == xt.shape[1]\n",
    "    cat_train = cat.Pool(x.to_pandas(), y.to_pandas())\n",
    "    cat_eval = cat.Pool(xt.to_pandas(), yt.to_pandas())\n",
    "    \n",
    "    clf = CatBoostRegressor(iterations=3000, \n",
    "                             task_type='GPU',\n",
    "                             bagging_temperature = 0.2,\n",
    "                             od_type='Iter',\n",
    "                             metric_period = 50,\n",
    "                             od_wait=20)\n",
    "    clf.fit(cat_train, eval_set=cat_eval, verbose=100,early_stopping_rounds=500)\n",
    "    return  clf.predict(cat_eval), clf"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-18T08:07:03.112945Z",
     "iopub.execute_input": "2022-08-18T08:07:03.113369Z",
     "iopub.status.idle": "2022-08-18T08:07:03.122426Z",
     "shell.execute_reply.started": "2022-08-18T08:07:03.113338Z",
     "shell.execute_reply": "2022-08-18T08:07:03.121359Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "import json\n",
    "learning_rate_init = 0.02\n",
    "epochs = 50\n",
    "def lr_scheduler(epoch):\n",
    "    if epoch <= epochs*0.8:\n",
    "        return learning_rate_init\n",
    "    else:\n",
    "        return learning_rate_init * 0.1\n",
    "def get_model(features):\n",
    "    inp = tf.keras.layers.Input((features,))\n",
    "    x = tf.keras.layers.Reshape((features,1))(inp)\n",
    "    x = tf.keras.layers.Conv1D(64,5,strides=5, activation='elu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv1D(32,1, activation='elu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv1D(16,1, activation='elu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv1D(4,1, activation='elu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    return tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "\n",
    "def CNN_train(x, y, xt, yt,features):\n",
    "    print(\"-----------CNN starts training-----------\")\n",
    "    callbacks = []\n",
    "    callbacks.append(tf.keras.callbacks.LearningRateScheduler(lr_scheduler))\n",
    "    optimizer = tf.keras.optimizers.Adam(lr = learning_rate_init, decay = 0.00001)\n",
    "    model = get_model(features)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    model.fit(x.to_pandas(), y.to_pandas(), validation_data=(xt.to_pandas(), yt.to_pandas()), epochs=50, verbose=2, batch_size=256,callbacks=callbacks)\n",
    "\n",
    "\n",
    "    return model.predict(xt.to_pandas(),batch_size=256), model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-18T08:07:03.141884Z",
     "iopub.execute_input": "2022-08-18T08:07:03.144351Z",
     "iopub.status.idle": "2022-08-18T08:07:10.147840Z",
     "shell.execute_reply.started": "2022-08-18T08:07:03.144303Z",
     "shell.execute_reply": "2022-08-18T08:07:10.146636Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def lgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label()), True\n",
    "\n",
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "# Created by https://www.kaggle.com/yunchonggan\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "# we still need the official metric since the faster version above is slightly off\n",
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "def amex_metric_mod_lgbm(y_pred: np.ndarray, data: lgb.Dataset):\n",
    "\n",
    "    y_true = data.get_label()\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 'AMEX', 0.5 * (gini[1]/gini[0]+ top_four), True"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-18T08:07:10.150752Z",
     "iopub.execute_input": "2022-08-18T08:07:10.151608Z",
     "iopub.status.idle": "2022-08-18T08:07:10.416743Z",
     "shell.execute_reply.started": "2022-08-18T08:07:10.151557Z",
     "shell.execute_reply": "2022-08-18T08:07:10.415105Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_17/1218050219.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# Created by https://www.kaggle.com/yunchonggan\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;31m# https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0;32mdef\u001B[0m \u001B[0mamex_metric_np\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpreds\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mfloat\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m     \u001B[0mindices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margsort\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpreds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0mpreds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpreds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "#(458913, 1123)(458913, 1156)\n",
    "path = '../input/amex-data-integer-dtypes-parquet-format'\n",
    "train = load_train(path)\n",
    "# train2 = load_train(path,2)\n",
    "train.shape"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-18T08:07:10.417679Z",
     "iopub.status.idle": "2022-08-18T08:07:10.418075Z",
     "shell.execute_reply.started": "2022-08-18T08:07:10.417878Z",
     "shell.execute_reply": "2022-08-18T08:07:10.417895Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "not_used = get_not_used()\n",
    "not_used = [i for i in not_used if i in train.columns]\n",
    "msgs = {}\n",
    "folds = 5\n",
    "score = 0\n",
    "\n",
    "#set diff cols if u wanna try different features on xgb & lgbm\n",
    "#diff_cols= [col for col in train.columns if col.endswith('_max') or col.endswith('_min') or col.endswith('_mean') or col.endswith('_std') or col.endswith('_count')]\n",
    "\n",
    "\n",
    "for i in range(folds):\n",
    "    print(f\"==============Folds {i}===============\")\n",
    "    mask = train['cid']%folds == i\n",
    "    tr,va = train[~mask], train[mask]\n",
    "\n",
    "    x, y = tr.drop(not_used, axis=1), tr['target']\n",
    "    xt, yt = va.drop(not_used, axis=1), va['target']\n",
    "    features = len(x.columns)\n",
    "    \n",
    "    xp, yp, bst = xgb_train(x, y, xt, yt)\n",
    "    bst.save_model(f'xgb_{i}.json')\n",
    "\n",
    "    x = tr.drop(not_used+diff_cols, axis=1)\n",
    "    xt = va.drop(not_used+diff_cols, axis=1)\n",
    "    \n",
    "    xp2,yp2,gbm = lgb_train(x, y, xt, yt)\n",
    "    gbm.save_model(f'lgb_{i}.json')\n",
    "    \n",
    "    yp3,cats = cat_train(x, y, xt, yt)\n",
    "    cats.save_model(f'cat_{i}.json')\n",
    "    \n",
    "    yp4,cnn = CNN_train(x, y, xt, yt,features)\n",
    "    model_json = cnn.to_json()\n",
    "    # 写入json文件\n",
    "    with open(f'cnn_train_{i}.json', 'w') as f:\n",
    "        json.dump(model_json, f)\n",
    "    preds = yp * 0.35+yp2 * 0.45+yp3 * 0.1+yp4 * 0.1\n",
    "    amex_score = amex_metric(pd.DataFrame({'target':yt.values.get()}), \n",
    "                                    pd.DataFrame({'prediction':preds}))\n",
    "    msg = f\"Fold {i} amex {amex_score:.4f}\"\n",
    "    print(msg)\n",
    "    score += amex_score\n",
    "    del tr,va,x,y\n",
    "    del xt,yt,cnn,cats,gbm\n",
    "    _ = gc.collect()\n",
    "score /= folds\n",
    "print(f\"Average amex score: {score:.4f}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-18T08:07:10.419123Z",
     "iopub.status.idle": "2022-08-18T08:07:10.419507Z",
     "shell.execute_reply.started": "2022-08-18T08:07:10.419321Z",
     "shell.execute_reply": "2022-08-18T08:07:10.419339Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "del train\n",
    "gc.collect()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-18T08:07:10.421364Z",
     "iopub.status.idle": "2022-08-18T08:07:10.421747Z",
     "shell.execute_reply.started": "2022-08-18T08:07:10.421556Z",
     "shell.execute_reply": "2022-08-18T08:07:10.421574Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "cids = []\n",
    "yps = []\n",
    "# set chunks \n",
    "chunks = 15\n",
    "\n",
    "\n",
    "for df in tqdm(load_test_iter(path,chunks),total=chunks):\n",
    "    cids.append(df['customer_ID'])\n",
    "    not_used = [i for i in not_used if i in df.columns]\n",
    "\n",
    "    preds=0\n",
    "    for i in range(folds):\n",
    "        bst = xgb.Booster()\n",
    "        bst.load_model(f'xgb_{i}.json')\n",
    "        dx = xgb.DMatrix(df.drop(not_used, axis=1))\n",
    "        \n",
    "        gbm = lgb.Booster(model_file=f'lgb_{i}.json')\n",
    "        dx2 = df.drop(not_used, axis=1).to_pandas()\n",
    "        \n",
    "        cats.load_model(f'cat_{i}.json')\n",
    "        \n",
    "        with open(r'cnn_train_{i}.json', 'r') as f:\n",
    "             model_json = json.load(f)\n",
    "        cnn = tf.keras.models.model_from_json(model_json)\n",
    "        \n",
    "        yp = bst.predict(dx, iteration_range=(0,bst.best_ntree_limit))\n",
    "        yp2 = gbm.predict(dx2, num_iteration =gbm.best_iteration)\n",
    "        yp3 = cats.predict(dx2)\n",
    "        yp4 = cnn.predict(dx2)\n",
    "        #preds+=final_estimator.predict_proba(np.concatenate((np.expand_dims(yp, 1), np.expand_dims(yp2, 1)), 1))[:,1]\n",
    "        preds+=(yp * 0.35+yp2 * 0.45+yp3 * 0.1+yp4 * 0.1)\n",
    "    yps.append(preds/folds)\n",
    "    \n",
    "df = cudf.DataFrame()\n",
    "df['customer_ID'] = cudf.concat(cids)\n",
    "df['prediction'] = np.concatenate(yps)\n",
    "df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-18T08:07:10.423412Z",
     "iopub.status.idle": "2022-08-18T08:07:10.424459Z",
     "shell.execute_reply.started": "2022-08-18T08:07:10.424245Z",
     "shell.execute_reply": "2022-08-18T08:07:10.424268Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sub = pd.read_csv('../input/stacking-first/submission1.csv')\n",
    "sub.to_csv('submission.csv', index = False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-18T08:08:03.215486Z",
     "iopub.execute_input": "2022-08-18T08:08:03.215938Z",
     "iopub.status.idle": "2022-08-18T08:08:03.318110Z",
     "shell.execute_reply.started": "2022-08-18T08:08:03.215898Z",
     "shell.execute_reply": "2022-08-18T08:08:03.316436Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_17/2689053665.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msub\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../input/stacking-first/submission1.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0msub\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'submission.csv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    480\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    481\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 482\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    483\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    484\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 811\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    812\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1038\u001B[0m             )\n\u001B[1;32m   1039\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1040\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1041\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m    227\u001B[0m             \u001B[0mmemory_map\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"memory_map\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m             \u001B[0mstorage_options\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"storage_options\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 229\u001B[0;31m             \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"encoding_errors\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"strict\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    230\u001B[0m         )\n\u001B[1;32m    231\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    705\u001B[0m                 \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    706\u001B[0m                 \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 707\u001B[0;31m                 \u001B[0mnewline\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    708\u001B[0m             )\n\u001B[1;32m    709\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../input/stacking-first/submission1.csv'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/stacking-first/submission1.csv'",
     "output_type": "error"
    }
   ]
  }
 ]
}
